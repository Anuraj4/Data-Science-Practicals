{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4ea8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem Statement:- Extract Sample document and apply following document preprocessing methods: Tokenization, POS \n",
    "# Tagging, stop words removal, Stemming and Lemmatization.\n",
    "# Create representation of documents by calculating Term Frequency and Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25412bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:-\n",
    "\n",
    "# Tokenization is the process of breaking down a text into smaller units called tokens.\n",
    "# These tokens can be words, sentences, or even subwords, depending on the level of granularity you need\n",
    "# for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ac1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Word tokenization:- Imagine if you have the chunk of text like paragraph\n",
    "# work tokenize is like breaking this text into smaller pieces.\n",
    "\n",
    "# 2) Sentence Tokenization: Similar to word tokenization, but instead of breaking\n",
    "# the text into individual words, you're breaking it into sentences.\n",
    "\n",
    "# 3) POS Tagging:- POS Tagging (Part-of-Speech Tagging): In natural language processing,\n",
    "# each word in a sentence can be categorized into a particular part of speech, such as noun, verb, adjective, etc.\n",
    "# For example, in the sentence \"The cat is sleeping\", POS tagging would label \"The\" as a determiner, \"cat\" as a noun, \"is\" as a verb, and \"sleeping\" as a verb.\n",
    "\n",
    "# 4) Stemming:- Stemming: Stemming is the process of reducing words to their root or base form, which allows different\n",
    "# variations of the same word to be treated as the same word. For example, \"running\", \"runs\", and \"ran\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28eaba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In summary, word tokenization breaks text into individual words, sentence tokenization breaks text into individual sentences, \n",
    "# POS tagging assigns grammatical labels to words in a sentence, and stemming reduces words to their base form. \n",
    "# These are all important preprocessing steps in natural language processing tasks.\n",
    "# NLTK library is used for the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3609741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is word-wise tokenization-:\n",
      " ['Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'of', 'Engineering', ',', 'Management', ',', 'and', 'Research', '.'] \n",
      "\n",
      "----------------------------------------------------------\n",
      "\n",
      "This is sentence-wise tokenization-:\n",
      " ['Dr. D. Y. Patil Institute of Engineering, Management, and Research.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "block = \"Dr. D. Y. Patil Institute of Engineering, Management, and Research.\"\n",
    "print(\"This is word-wise tokenization-:\\n\", nltk.word_tokenize(block), '\\n')\n",
    "print(\"----------------------------------------------------------\\n\")\n",
    "print(\"This is sentence-wise tokenization-:\\n\", nltk.sent_tokenize(block))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "738e62cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "This is the unclean version-: \n",
      " ['Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'of', 'Engineering', ',', 'Management', ',', 'and', 'Research', '.'] \n",
      "\n",
      "----------------------------------------------------------------------'\n",
      "'\n",
      "This is the cleaned version-: \n",
      " ['Dr.', 'D.', 'Y.', 'Patil', 'Institute', 'Engineering', ',', 'Management', ',', 'Research', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "# `stop_words = stopwords.words('english')` is creating a list of English stopwords using the NLTK library.\n",
    "# Stopwords are common words (such as 'and', 'the', 'is', etc.) that are often filtered out from text\n",
    "# data during natural language processing tasks like text analysis or text mining. These words are\n",
    "# considered to be non-informative and are typically removed to focus on the more meaningful words\n",
    "# in the text.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(stop_words)\n",
    "token = nltk.word_tokenize(block)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "print(\"This is the unclean version-:\", \"\\n\", token, \"\\n\")\n",
    "print(\"----------------------------------------------------------------------'\\n'\")\n",
    "print(\"This is the cleaned version-:\", \"\\n\", cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb267c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = nltk.PorterStemmer()\n",
    "words = ['rain', 'rained', 'raining', 'rains']\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca788f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')#data dependencies\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in cleaned_token]\n",
    "print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged = nltk.pos_tag(cleaned_token)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d732f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_1 = \"Our aim is to develop a good work culture among students, a culture where students from various technical backgrounds come together to teach,guide and collaborate with each other on various projects and grow together.\"\n",
    "block_2 = \"Keeping in mind the interest of the IT professionals and computer enthusiasts, CSI works towards making the profession an area of choice amongst all sections of the society. The promotion of Information Technology as a profession is the top priority of CSI today. To fulfill this objective,the CSI regularly organizes conferences, conventions, lectures, projects,and awards. And at the same time, it also ensures that regular training and skill updating are organized for the future IT professionals.\"\n",
    "#split so each word have their own string\n",
    "first_block = block_1.split(\" \")\n",
    "second_block = block_2.split(\" \")\n",
    "#join them to remove common duplicate words\n",
    "total= set(first_block).union(set(second_block))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490639fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0)\n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_block:\n",
    "    wordDictA[word]+=1\n",
    "for word in second_block:\n",
    "    wordDictB[word]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d99840",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac37436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in wordDictA if not w in stop_words]\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_block)\n",
    "tfSecond = computeTF(wordDictB, second_block)\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items(): idfDict[word] = math.log10(N /(float(val) + 1))\n",
    "    return(idfDict)\n",
    "\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "idfs1 = pd.DataFrame([wordDictA, wordDictB])\n",
    "print(idfs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items(): tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "#putting it in a dataframe\n",
    "idf= pd.DataFrame([idfFirst, idfSecond])\n",
    "print(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab0956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1cc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07984b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf0716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The WordNet is a part of Python's Natural Language Toolkit. It is a large word database of English Nouns, Adjectives, Adverbs and Verbs. \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e727cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Make sure all words are in lowercase\n",
    "version_1 = \"Developing a competitive culture where the students polish technical and professional attributes, gain experience and learn new skills while upgrading the already present skillset. For those fledglings who have a zeal to build a strong profile and are hunting for their Ikigai, CSI provides ample opportunities for those individuals too.\"\n",
    "version_2 = \"Personalized career guidance, Regular Logic and aptitude building activities, Industrial level project collaboration, Building a network with active collaborations across the globe, Periodic member exclusive conferences and seminars, Created a community for sharing skills and knowledge\"\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([version_1.lower(), version_2.lower()])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
